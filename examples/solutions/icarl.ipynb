{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from mammoth_lite import register_model, ContinualModel, load_runner, train, add_rehearsal_args, Buffer, ContinualDataset, MammothBackbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05697e37",
   "metadata": {},
   "source": [
    "# Filling the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b88f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_balanced_buffer(buffer: Buffer, dataset: ContinualDataset, t_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    Adds examples from the current task to the memory buffer.\n",
    "\n",
    "    Args:\n",
    "        buffer: the memory buffer\n",
    "        dataset: the dataset from which take the examples\n",
    "        t_idx: the task index\n",
    "    \"\"\"\n",
    "\n",
    "    n_seen_classes = dataset.N_CLASSES_PER_TASK * (t_idx + 1)\n",
    "    n_past_classes = dataset.N_CLASSES_PER_TASK * t_idx\n",
    "    samples_per_class = buffer.buffer_size // n_seen_classes\n",
    "\n",
    "    mask = dataset.train_loader.dataset.targets >= n_past_classes\n",
    "    dataset.train_loader.dataset.targets = dataset.train_loader.dataset.targets[mask]\n",
    "    dataset.train_loader.dataset.data = dataset.train_loader.dataset.data[mask]\n",
    "\n",
    "    if t_idx > 0:\n",
    "        # 1) First, subsample prior classes\n",
    "        buf_data = buffer.examples\n",
    "        buf_y = buffer.labels\n",
    "\n",
    "        buffer.reset() # clear the buffer before filling it\n",
    "\n",
    "        # Fill the buffer with samples from prior classes up to `samples_per_class`\n",
    "        # This is done to ensure that the buffer has a balanced number of samples per class\n",
    "        for _y in buf_y.unique():\n",
    "            cls_idxs = (buf_y == _y)\n",
    "            _buf_data_idx = buf_data[cls_idxs][:samples_per_class]\n",
    "            buffer.add_data(examples=_buf_data_idx,\n",
    "                            labels=buf_y[cls_idxs][:samples_per_class])\n",
    "\n",
    "    examples, labels = dataset.train_loader.dataset.data, dataset.train_loader.dataset.targets\n",
    "\n",
    "    # 2) Fill the buffer with samples from the current class\n",
    "    for _y in np.unique(labels):\n",
    "        cls_idxs = (labels == _y)\n",
    "        _x, _y = examples[cls_idxs], labels[cls_idxs]\n",
    "\n",
    "        # Add only up to `samples_per_class` examples per class\n",
    "        buffer.add_data(\n",
    "            examples=_x[:samples_per_class],\n",
    "            labels=_y[:samples_per_class]\n",
    "        )\n",
    "\n",
    "    # NOTE: if this fails and you comment this it still works but the buffer will not be balanced\n",
    "    assert len(buffer.examples) <= buffer.buffer_size, f\"buffer overflowed its maximum size: {len(buffer)} > {buffer.buffer_size}\"\n",
    "    assert buffer.num_seen_examples <= buffer.buffer_size, f\"buffer has been overfilled, there is probably an error: {buffer.num_seen_examples} > {buffer.buffer_size}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1b1f2",
   "metadata": {},
   "source": [
    "## Bonus: fill the buffer with Herding\n",
    "\n",
    "Herding selects the examples that are the closest from the average feature representation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f45f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_buffer_with_herding(buffer: Buffer, dataset: ContinualDataset, t_idx: int, net: MammothBackbone) -> None:\n",
    "    \"\"\"\n",
    "    Adds examples from the current task to the memory buffer **with Herding**.\n",
    "\n",
    "    Args:\n",
    "        buffer: the memory buffer\n",
    "        dataset: the dataset from which take the examples\n",
    "        t_idx: the task index\n",
    "        net: the model instance\n",
    "    \"\"\"\n",
    "    mode = net.training\n",
    "    net.eval()\n",
    "    device = next(net.parameters()).device\n",
    "\n",
    "    n_seen_classes = dataset.N_CLASSES_PER_TASK * (t_idx + 1)\n",
    "    n_past_classes = dataset.N_CLASSES_PER_TASK * t_idx\n",
    "    samples_per_class = buffer.buffer_size // n_seen_classes\n",
    "\n",
    "    mask = dataset.train_loader.dataset.targets >= n_past_classes\n",
    "    dataset.train_loader.dataset.targets = dataset.train_loader.dataset.targets[mask]\n",
    "    dataset.train_loader.dataset.data = dataset.train_loader.dataset.data[mask]\n",
    "\n",
    "    if t_idx > 0:\n",
    "        # 1) First, subsample prior classes\n",
    "        buf_data = buffer.examples\n",
    "        buf_y = buffer.labels\n",
    "\n",
    "        buffer.reset() # clear the buffer before filling it\n",
    "\n",
    "        # Fill the buffer with samples from prior classes up to `samples_per_class`\n",
    "        # This is done to ensure that the buffer has a balanced number of samples per class\n",
    "        for _y in buf_y.unique():\n",
    "            cls_idxs = (buf_y == _y)\n",
    "            _buf_data_idx = buf_data[cls_idxs][:samples_per_class]\n",
    "            buffer.add_data(examples=_buf_data_idx,\n",
    "                            labels=buf_y[cls_idxs][:samples_per_class])\n",
    "            \n",
    "    norm_trans = dataset.get_normalization_transform()\n",
    "\n",
    "    # 2 Extract all features\n",
    "    examples, labels, features = [], [], []\n",
    "    for data in dataset.train_loader:\n",
    "        x, y, not_norm_x = data[0], data[1], data[2]\n",
    "        if not x.size(0):\n",
    "            continue\n",
    "        examples.append(not_norm_x.cpu())\n",
    "        labels.append(y.cpu())\n",
    "\n",
    "        # Compute the features for the current batch\n",
    "        feats = net(norm_trans(not_norm_x.to(device)), returnt='features')\n",
    "        features.append(feats.cpu())\n",
    "\n",
    "    examples, labels, features = torch.cat(examples), torch.cat(labels), torch.cat(features)\n",
    "\n",
    "    # 3 Fill the buffer with samples from the current class using Herding\n",
    "    # Herding is a greedy method to select samples that are representative of the class\n",
    "    # It selects samples that minimize the distance to the mean feature of the class\n",
    "    for _y in labels.unique():\n",
    "        cls_idxs = (labels == _y)\n",
    "        _x, _y, feats = examples[cls_idxs], labels[cls_idxs], features[cls_idxs]\n",
    "\n",
    "        # Herding step 1: starting from the mean feature of the class\n",
    "        mean_class_feat = feats.mean(0, keepdim=True)\n",
    "\n",
    "        # Herding step 2: ... and an empty running sum\n",
    "        running_sum = torch.zeros_like(mean_class_feat)\n",
    "        i = 0\n",
    "\n",
    "        while i < samples_per_class and i < feats.shape[0]:\n",
    "\n",
    "            # Herding step 3: Compute the cost as the distance to the mean feature\n",
    "            # The cost defines which sample we should add to the buffer such that\n",
    "            # the running mean of the features is as close as possible to the mean feature\n",
    "            running_mean = (feats + running_sum) / (i + 1)\n",
    "\n",
    "            # Compute the cost as the L2 norm between the mean feature and the running mean\n",
    "            cost = (mean_class_feat - running_mean).norm(2, 1)\n",
    "\n",
    "            # Herding step 4: Select the sample with the minimum cost\n",
    "            idx_min = cost.argmin().item()\n",
    "\n",
    "            # Herding step 5: Add the sample to the buffer\n",
    "            buffer.add_data(\n",
    "                examples=_x[idx_min:idx_min + 1],\n",
    "                labels=_y[idx_min:idx_min + 1]\n",
    "            )\n",
    "\n",
    "            running_sum += feats[idx_min:idx_min + 1]\n",
    "            feats[idx_min] = feats[idx_min] + 1e6\n",
    "            i += 1\n",
    "\n",
    "    assert len(buffer.examples) <= buffer.buffer_size, f\"buffer overflowed its maximum size: {len(buffer)} > {buffer.buffer_size}\"\n",
    "    assert buffer.num_seen_examples <= buffer.buffer_size, f\"buffer has been overfilled, there is probably an error: {buffer.num_seen_examples} > {buffer.buffer_size}\"\n",
    "\n",
    "    net.train(mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icarl_replay(self: ContinualModel, dataset: ContinualDataset, current_task: int):\n",
    "    \"\"\"\n",
    "    Merge the replay buffer with the current task data.\n",
    "\n",
    "    Args:\n",
    "        self: the model instance\n",
    "        dataset: the dataset\n",
    "        current_task: the current task index\n",
    "    \"\"\"\n",
    "\n",
    "    if current_task > 0:\n",
    "        data_concatenate = torch.cat if isinstance(dataset.train_loader.dataset.data, torch.Tensor) else np.concatenate\n",
    "        def refold_transform(x):\n",
    "            return (x.cpu() * 255).permute([0, 2, 3, 1]).numpy().astype(np.uint8)\n",
    "        \n",
    "        # REDUCE AND MERGE TRAINING SET\n",
    "        dataset.train_loader.dataset.targets = np.concatenate([\n",
    "            dataset.train_loader.dataset.targets,\n",
    "            self.buffer.labels.cpu().numpy()[:len(self.buffer)]\n",
    "        ])\n",
    "        dataset.train_loader.dataset.data = data_concatenate([\n",
    "            dataset.train_loader.dataset.data,\n",
    "            refold_transform(self.buffer.examples[:len(self.buffer)])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef5727",
   "metadata": {},
   "source": [
    "# iCaRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@register_model('icarl')\n",
    "class iCaRL(ContinualModel):\n",
    "    COMPATIBILITY = ['class-il', 'task-il']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_parser(parser) -> ArgumentParser:\n",
    "        add_rehearsal_args(parser)\n",
    "        parser.add_argument('--opt_wd', type=float, default=1e-5,\n",
    "                            help='Optimizer weight decay')\n",
    "        parser.add_argument('--use_herding', type=int, default=1, choices=[0, 1],\n",
    "                            help='Use herding to fill the buffer')\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, backbone, loss, args, transform, dataset=None):\n",
    "        super().__init__(backbone, loss, args, transform, dataset=dataset)\n",
    "\n",
    "        # Instantiate buffer\n",
    "        self.buffer = Buffer(self.args.buffer_size)\n",
    "\n",
    "        self.eye = torch.eye(self.dataset.N_CLASSES_PER_TASK *\n",
    "                             self.dataset.N_TASKS).to(self.device)\n",
    "\n",
    "        self.class_means = None\n",
    "        self.old_net = None\n",
    "        self.current_task = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.class_means is None:\n",
    "            with torch.no_grad():\n",
    "                self.compute_class_means()\n",
    "                self.class_means = self.class_means.squeeze()\n",
    "\n",
    "        # Compute the features\n",
    "        feats = self.net(x, returnt='features')\n",
    "\n",
    "        feats = feats.view(feats.size(0), -1)\n",
    "\n",
    "        # Compute the nearest-mean-of-exemplars prediction (Eq. 2 of the iCaRL paper)\n",
    "        pred = (self.class_means.unsqueeze(0) - feats.unsqueeze(1)).pow(2).sum(2)\n",
    "        return -pred\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute_class_means(self) -> None:\n",
    "        \"\"\"\n",
    "        Computes a vector representing mean features for each class.\n",
    "        \"\"\"\n",
    "        was_training = self.net.training\n",
    "        self.net.eval()\n",
    "        transform = self.dataset.get_normalization_transform()\n",
    "        class_means = []\n",
    "        examples, labels = self.buffer.examples, self.buffer.labels\n",
    "        for _y in labels.unique():\n",
    "            x_buf = torch.stack(\n",
    "                [examples[i]\n",
    "                 for i in range(0, len(examples))\n",
    "                 if labels[i].cpu() == _y]\n",
    "            )\n",
    "\n",
    "            all_features = []\n",
    "            while len(x_buf):\n",
    "                batch = x_buf[:self.args.batch_size]\n",
    "                x_buf = x_buf[self.args.batch_size:]\n",
    "                \n",
    "                # Apply the normalization transform\n",
    "                batch = torch.stack([transform(x) for x in batch.cpu()]).to(self.device)\n",
    "\n",
    "                # Compute the features for the current batch\n",
    "                feats = self.net(batch, returnt='features')\n",
    "\n",
    "                all_features.append(feats)\n",
    "            \n",
    "            # Concatenate all features and compute the mean\n",
    "            all_features = torch.cat(all_features).mean(0)\n",
    "\n",
    "            class_means.append(all_features.flatten())\n",
    "        self.class_means = torch.stack(class_means)\n",
    "        self.net.train(was_training)\n",
    "\n",
    "    def end_task(self, dataset) -> None:\n",
    "        # Save the current model as the old model\n",
    "        self.net.eval()\n",
    "        self.old_net = deepcopy(self.net)\n",
    "\n",
    "        # Fill the buffer with examples from the current task\n",
    "        with torch.no_grad():\n",
    "            if self.args.use_herding:\n",
    "                fill_buffer_with_herding(self.buffer, dataset, self.current_task, net=self.net)\n",
    "            else:\n",
    "                fill_balanced_buffer(self.buffer, dataset, self.current_task)\n",
    "        self.class_means = None\n",
    "\n",
    "        self.current_task += 1\n",
    "\n",
    "    def begin_task(self, dataset):\n",
    "        # Concatenate the buffer with the current task data\n",
    "        icarl_replay(self, dataset, self.current_task)\n",
    "        self.net.train()\n",
    "\n",
    "    def get_loss(self, inputs: torch.Tensor, labels: torch.Tensor,\n",
    "                 task_idx: int, logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This is pretty much the same as LwF.MC\n",
    "        \"\"\"\n",
    "        previous_classes = task_idx * self.dataset.N_CLASSES_PER_TASK\n",
    "\n",
    "        # Compute the outputs of the current model\n",
    "        outputs = self.net(inputs)\n",
    "        if task_idx == 0:\n",
    "            # If this is the first task, we do not have any previous classes\n",
    "            targets = self.eye[labels]\n",
    "            # Compute the loss as binary cross-entropy\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs, targets)\n",
    "            assert loss >= 0\n",
    "        else:\n",
    "            # If this is not the first task, we have previous classes\n",
    "            targets = self.eye[labels]\n",
    "            # We concatenate the logits of the previous classes with the targets of the current task\n",
    "            comb_targets = torch.cat((logits[:, :previous_classes], targets[:, previous_classes:]), dim=1)\n",
    "            # Compute the loss as binary cross-entropy\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs, comb_targets)\n",
    "            assert loss >= 0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def observe(self, inputs, labels, not_aug_inputs, logits=None, epoch=None):\n",
    "        self.class_means = None\n",
    "        if self.current_task > 0:\n",
    "            with torch.no_grad():\n",
    "                # Compute the output for the old model\n",
    "                logits = torch.sigmoid(self.old_net(inputs))\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss = self.get_loss(inputs, labels, self.current_task, logits)\n",
    "\n",
    "        # Add weight decay to the loss\n",
    "        if self.args.opt_wd > 0:\n",
    "            loss = loss + torch.sum(self.net.get_params() ** 2) * self.args.opt_wd\n",
    "        loss.backward()\n",
    "\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can use the `load_runner` function to load our custom model.\n",
    "\"\"\"\n",
    "args = {\n",
    "    'lr': 0.1, \n",
    "    'n_epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'buffer_size': 500,\n",
    "    'opt_wd': 1e-5,\n",
    "    'use_herding': 1,\n",
    "    }\n",
    "\n",
    "model, dataset = load_runner('icarl','seq-cifar10',args)\n",
    "train(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
