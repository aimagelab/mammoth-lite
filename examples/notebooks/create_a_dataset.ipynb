{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfab52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this script we will define a new custom dataset to be used with the Mammoth framework.\n",
    "\n",
    "We will need:\n",
    "- The `register_dataset` function to register our dataset.\n",
    "- The `ContinualDataset` basas class to inherit from.\n",
    "- The `MammothDataset` class to implement the dataset.\n",
    "\n",
    "In addition, we will use the `base_path` function to get the path where the dataset files will be stored,\n",
    "  and the `load_runner` and `train` functions to run our training process.\n",
    "\"\"\"\n",
    "\n",
    "from mammoth_lite import register_dataset, ContinualDataset, MammothDataset, load_runner, train, base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4473dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before defining a Continual Learning dataset to use in Mammoth, we need a data source. \n",
    "In Mammoth this is usually done by craeting a \"joint\" dataset, which is a dataset that contains all the data from all tasks.\n",
    "This dataset will be then split into tasks later on.\n",
    "We will use the CIFAR10 dataset as our data source in this example.\n",
    "\n",
    "The source dataset SHOULD be a subclass of `torch.utils.data.Dataset` (or implement the required `__len__` and `__getitem__` methods).\n",
    "To make it easier to develop and allow type suggestions, we suggest to inherit from `MammothDataset` which is a subclass of `torch.utils.data.Dataset`.\n",
    "\n",
    "NOTE: if you do not use `MammothDataset`, be sure your dataset contains the `not_aug_transform` attribute, which is a transformation that does not apply any data augmentation.\n",
    "\n",
    "In addition, the dataset MUST define:\n",
    "- `data` and `targets` attributes, which contain the training/testing data and labels respectively.\n",
    "- `not_aug_transform` attribute, which is a transformation that does not apply any data augmentation.\n",
    "- `__getitem__` method, which returns a tuple of (image, label, not_aug_image) where:\n",
    "    - `image` is the transformed image (with data augmentation applied).\n",
    "    - `label` is the label of the image.\n",
    "    - `not_aug_image` is the original image without any data augmentation applied.\n",
    "\n",
    "The `not_aug_image` is used by rehearsal methods to store the original image without any data augmentation applied.\n",
    "The presence of this attribute is also the main reason why we cannot simply use the `torchvision.datasets.CIFAR10` dataset directly, as it returns only the transformed image and label.\n",
    "\"\"\"\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MammothCIFAR10(MammothDataset, CIFAR10):\n",
    "    \"\"\"\n",
    "    Overrides the CIFAR10 dataset to change the getitem function.\n",
    "\n",
    "    The CIFAR10 dataset already contains the data and targets attributes, so we do not need to redefine them.\n",
    "    In addition, the MammothDataset class already contains the not_aug_transform attribute, so we do not need to redefine it either.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, is_train=True, transform=None) -> None:\n",
    "        \"\"\"\n",
    "        Implementing the constructor is not strictly necessary, but it is usually required to load the data and targets in more practical scenarios where data does not simply come from torchvision.\n",
    "        \"\"\"\n",
    "        # the `not self._check_integrity()` is just a trick to avoid printing debug messages\n",
    "        self.root=root\n",
    "        super(MammothCIFAR10, self).__init__(root, is_train, transform, download=not self._check_integrity())        \n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Gets the requested element from the dataset.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # In order to apply data augmentation, we need to convert the image from a numpy array to a PIL Image.\n",
    "        img = Image.fromarray(img, mode='RGB')\n",
    "        original_img = img.copy() # if you do not copy the image, the original image will be modified by the data augmentation transformations.\n",
    "\n",
    "        # Apply the not_aug_transform to get the original image without any data augmentation.\n",
    "        not_aug_img = self.not_aug_transform(original_img)\n",
    "\n",
    "        # Apply the transform to get the augmented image.\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target, not_aug_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a2475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_dataset(name='custom-cifar10')\n",
    "class CustomSeqCifar10(ContinualDataset):\n",
    "    \"\"\"\n",
    "    This is the main class that defines a custom Continual Learning dataset in Mammoth.\n",
    "    It MUST inherit from `ContinualDataset` and implement the required attributes and methods.\n",
    "\n",
    "    The required attributes are:\n",
    "    - NAME: name of the dataset.\n",
    "    - SETTING: setting of the dataset. In Mammoth-Lite, this is usually 'class-il' for class incremental learning. In Mammoth, this can also be 'domain-il' for domain incremental learning, 'general-continual' for general continual learning, and others.\n",
    "    - N_CLASSES_PER_TASK: number of classes for each task. Here we assume that all tasks have the same number of classes. In Mammoth, this can also be a list of integers, where each integer represents the number of classes for each task.\n",
    "    - N_TASKS: number of tasks.\n",
    "    - MEAN: tuple of means for each channel of the dataset.\n",
    "    - STD: tuple of standard deviations for each channel of the dataset.\n",
    "    - TRANSFORM: torchvision transform to apply to the dataset during *training*.\n",
    "    - TEST_TRANSFORM: torchvision transform to apply to the dataset during *testing*.\n",
    "\n",
    "    In addition, it MUST implement the `get_data_loaders` method that returns the train and test datasets and the `get_backbone` method that returns the name of the backbone architecture to use for training.\n",
    "    These datasets will be those that we defined earlier, which inherit from `MammothDataset` and implement the required methods.\n",
    "\n",
    "    NOTE: We are not defining the constructor of this class, as it is not strictly necessary. \n",
    "    This also means that every time we do not have a way to dynamically customize the dataset, such as loading it from a different path or applying different transformations, we will need to create a new class that inherits from `ContinualDataset` and implements the required attributes and methods. In the full Mammoth framework there are several strategies to handle this, including adding any parameters to the `__init__` method, which will be available via CLI.\n",
    "    \"\"\"\n",
    "\n",
    "    NAME = 'seq-cifar10'\n",
    "    SETTING = 'class-il'\n",
    "    N_CLASSES_PER_TASK = 2\n",
    "    N_TASKS = 5\n",
    "    MEAN, STD = (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2615)\n",
    "    TRANSFORM = transforms.Compose(\n",
    "        [transforms.RandomCrop(32, padding=4),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(MEAN, STD)])\n",
    "    TEST_TRANSFORM = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n",
    "\n",
    "    def get_data_loaders(self):\n",
    "        \"\"\"\n",
    "        Class method that returns the train and test loaders.\n",
    "        \"\"\"\n",
    "        train_dataset = MammothCIFAR10(base_path() + 'CIFAR10', is_train=True, transform=self.TRANSFORM)\n",
    "        test_dataset = MammothCIFAR10(base_path() + 'CIFAR10', is_train=False, transform=self.TEST_TRANSFORM)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone():\n",
    "        \"\"\"\n",
    "        The name of a registered backbone (see `create_a_backbone.ipynb` for more details).  \n",
    "        \"\"\"\n",
    "        return \"resnet18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d23865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model:  sgd\n",
      "- Using ResNet as backbone\n",
      "Using device cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d86543f07564c7ca80adcaf60741a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Task 1: 100%|██████████| 63/63 [00:00<00:00, 86.66it/s, acc_task_1=87.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for task 1\t[Class-IL]: 87.60 \t[Task-IL]: 87.60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae124bb13b8e4be588d0ebf91908be74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we can use the `load_runner` function to load our model on the custom dataset.\n",
    "\"\"\"\n",
    "\n",
    "model, dataset = load_runner('sgd','custom-cifar10',{'lr': 0.1, 'n_epochs': 1, 'batch_size': 32})\n",
    "train(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a857877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
